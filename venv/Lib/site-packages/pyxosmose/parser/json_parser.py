import pandas as pd
import json
from pathlib import Path, PurePath

pd.set_option("expand_frame_repr", False)
pd.set_option('display.max_rows', None)
pd.options.mode.chained_assignment = None



class Parser:

    def __init__(self, file):
        if isinstance(file, PurePath):
            with open(file, "r", encoding='utf-8') as f:
                self.json = json.load(f)

        self.cluster = Clusters(self.json)
        self.cluster.to_csv(Path('osm_csv', Path(file).stem))

        self.model = Model(self.json)
        self.model.to_csv(Path('osm_csv', Path(file).stem))

        self.result = Results(self.json, self.model.units)
        self.result.to_csv(Path('osm_csv', Path(file).stem))

        # self.logFile = Path("Parser_report.Rmd")
        # self.logFile.unlink(missing_ok=True)
        #
        # self.layer_error = pd.DataFrame()
        # self.demand_error = pd.DataFrame()
        # self.supply_error = pd.DataFrame()
        #
        #
        # self.model = Model()
        # self.header = pd.DataFrame()
        # self.layer = pd.DataFrame()
        # self.unit = pd.DataFrame()
        # self.unique_unit = pd.DataFrame()
        # self.unit_connection  = pd.DataFrame()
        # self.flow = pd.DataFrame()
        # self.demand = pd.DataFrame()
        # self.supply = pd.DataFrame()
        # self.impact = pd.DataFrame()


    def get_demand(self):
        data = self.json
        if self.layer.empty:
            self.layer = self.get_layer()
        # Demand
        df = pd.DataFrame(data["results"]["Units_demand"][0])
        df.index.name = 'unitFullName'
        df = df.reset_index()
        df[['unitCluster', 'unitLocation', 'modelUnitName']] = \
            df['unitFullName'].str.split('_', 2,
                                                expand=True)  # TODO: create another table with cluster/location and merge
        df.columns.name = 'layerName'
        df = df.set_index(['unitFullName', 'unitCluster', 'unitLocation', 'modelUnitName']).stack()
        df = df.reset_index().rename(columns={0: 'unitDemand'})
        df = pd.merge(df, self.layer[['layerName', 'layerUnits']], how="left", on=['layerName'])

        self.demand_error = df.duplicated(subset=['unitFullName', 'layerName'])
        df = df.drop_duplicates(subset=['unitFullName', 'layerName'], keep='first')  # get reide of duplicated layer phys.units
        return df
        # df = pd.merge(df, df_unique_unit, how="left", on=['modelUnitName'])
        # df = pd.merge(df, df_header[['modelName', 'displayName']], how="left", on=['modelName'])

    def get_supply(self):
        data = self.json
        if self.header.empty:
            self.header = self.header_df()
        if self.unique_unit.empty:
            self.unique_unit = self.get_unique_unit()
        if self.layer.empty:
            self.layer = self.get_layer()
        # Supply
        df = pd.DataFrame(data["results"]["Units_supply"][0])
        df.index.name = 'unitFullName'
        df = df.reset_index()
        df[['unitCluster', 'unitLocation', 'modelUnitName']] = \
            df['unitFullName'].str.split('_', 2, expand=True)
        df.columns.name = 'layerName'
        df = df.set_index(['unitFullName', 'unitCluster', 'unitLocation', 'modelUnitName']).stack()
        df = df.reset_index().rename(columns={0: 'unitSupply'})
        df = pd.merge(df, self.layer[['layerName', 'layerUnits']], how="left", on=['layerName'])

        self.supply_error = df.duplicated(subset=['unitFullName', 'layerName'])
        df = df.drop_duplicates(subset=['unitFullName', 'layerName'],
                                              keep='first')  # get reide of duplicated layer phys.units
        df = pd.merge(df, self.unique_unit, how="left", on=['modelUnitName'])
        df = pd.merge(df, self.header[['modelName', 'modelDisplayName']], how="left", on=['modelName'])
        return df

    def get_impact(self):
        data = self.json
        if self.header.empty:
            self.header = self.header_df()
        if self.unit.empty:
            self.unit = self.get_unit()
        d = {'DefaultImpact': 'impact',
             'DefaultInvCost': 'capex',
             'DefaultMechPower': 'mpower',
             'DefaultOpCost': 'opex'}
        df_list = list()
        for key in data["results"]["Units_Cost"][0].keys():
            df = pd.json_normalize(data["results"]["Units_Cost"][0][key]).transpose()
            df.index.name = 'unitFullName'
            df = df.reset_index().rename(columns={0: d[key]})
            df_list.append(df)
        df = df_list[0]
        for i in range(len(df_list) - 1):
            df = pd.merge(df, df_list[i + 1], how='outer', on='unitFullName')
        df[['unitCluster', 'unitLocation', 'modelUnitName']] = \
            df['unitFullName'].str.split('_', 2, expand=True)
        df = pd.merge(df, self.unit[['modelUnitName', 'modelName', 'unitName']],
                      how="left", on=['modelUnitName'])
        df = pd.merge(df, self.header[['modelName', 'modelDisplayName']], how="left", on=['modelName'])
        return df

    def get_rmd_report(self):
        # Model quality report
        print('Rmd report')
        # n_duplicates = sum()
        # if n_duplicates > 0:
        #     errstr = 'WARNING: STILL ' + str(n_duplicates) + ' PHYSICAL UNITS INCOHERENCY(IES) IN DEMAND LAYERS'
        # if n_duplicates > 0:
        #     errstr = 'WARNING: STIIL ' + str(n_duplicates) + ' PHYSICAL UNITS INCOHERENCY(IES) IN SUPPLY LAYERS'
        # if len(self.error_duplicated) > 0:
        #     # print('Physical units incoherencies detected for layer(s)')
        #     # print(set(error_duplicated['layerName']))
        #     self.logStr = '{}\n{}'.format(
        #         self.error_duplicated.to_csv(self.logFile, index=False, sep=','),
        #         self.logStr)
        #
        # with open(self.logFile, "a") as f:
        #     f.write(self.logStr)

    def parse(self):
        return {
            'header': self.model.model,
            'layers': self.model.layers,
            'streams': self.model.streams,
            'inputs': self.model.inputs,
            'units': self.model.units,
            'unit_connections': self.model.connections,
            'flow': self.result.flows,
            'massStreams': self.model.massStreams,
            'costStreams': self.model.costStreams,
            #'unit_definition': self.get_unit_definition(),
            #'demand': self.get_demand(),
            #'supply': self.get_supply(),
            #'impact': self.get_impact(),
        }

class Clusters():

    def __init__(self, json):
        self.clusters = pd.DataFrame()
        if "clusters" in json:
            df = pd.DataFrame(json["clusters"][0]).explode('locationsOfCluster').explode('unitsOfCluster')
            df['modelUnitName'] = [unitsOfCluster.replace(clusterName+'_'+locationsOfCluster+'_', '')
                                        for unitsOfCluster, clusterName,  locationsOfCluster in
                                        zip(df.unitsOfCluster.astype('str'), df.clusterName.astype('str'),
                                            df.locationsOfCluster.astype('str'))]
            df = df[~df['unitsOfCluster'].isna()] # if there is some unused locations
            self.clusters = df.set_index('modelUnitName', verify_integrity=True)

    def to_csv(self, directory):
        Path(directory).mkdir(parents=True, exist_ok=True)
        self.clusters.to_csv(Path(directory, 'clusters.csv'), sep='\t')


class Model():

    def __init__(self, json):
        self.json = json
        self.df = pd.DataFrame(json["model"]).stack()
        self.df.index.set_names(['key', 'modelName'], inplace=True)
        self.df = self.df.reset_index()

        self.model = self.model_df()
        self.layers, self.layers_error = self.layer_df()
        self.streams = self.stream_df()
        self.units = self.unit_df()
        self.heatStreams = self.heat_stream_df()
        self.costStreams = self.cost_stream_df()
        self.massStreams = self.mass_stream_df()
        self.connections = self.connection_df()
        self.inputs = self.input_df()

    def model_df(self):
        df = self.df[self.df['key'] == 'header'].drop(columns=['key']).set_index('modelName')
        df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series, dtype='object')], axis=1)
        columns = {'displayName' : 'modelDisplayName',
                   'abbreviation': 'modelAbbreviation',
                   'group'       : 'modelGroup',
                   'subgroup'    : 'modelSubgroup'}
        df.rename(columns=columns, inplace=True)
        return df

    def layer_df(self):
        df = self.df[self.df['key'] == 'layers'].drop(columns=['key']).set_index('modelName')
        df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series)], axis=1).stack()
        df = df.apply(pd.Series)
        df.index.set_names(['model', 'id'], inplace=True)
        df = df.reset_index().drop(columns=['id'])
        df.rename(columns={'model': 'modelName',
                                 'displayName': 'layerDisplayName',
                                 'name': 'layerName',
                                 'type': 'layerType',
                                 'unit': 'layerUnits'}, inplace=True)
        cols1 = ['layerDisplayName', 'layerName', 'layerType', 'layerUnits']
        df_unique = df.drop_duplicates(subset=cols1)
        cols2 = ['layerDisplayName', 'layerName', 'layerType']
        df_error = df_unique[df_unique.duplicated(subset=cols2, keep=False)].sort_values(by='layerName')
        df = df.drop_duplicates(subset=['layerName', 'layerType']).sort_values(by='layerName')
        return df.set_index(['layerName']), df_error


    def stream_df(self):
        # -> modelName, unitDisplayName, unitName, unitType, streamDisplayName, streamInOut, layerName, streamName, streamType, modelUnitName
        # df = self.df[self.df['key'] == 'units'].drop(columns=['key']).set_index('modelName').explode(0)
        # df = pd.concat([df.drop(0, axis=1), pd.DataFrame(df[0].values.tolist(), index=df.index)]).stack()
        df = self.df[self.df['key'] == 'units'].drop(columns=['key']).set_index('modelName')
        df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series, dtype='object')], axis=1).stack()
        df = df.apply(pd.Series)
        df.index.set_names(['modelName', 'id'], inplace=True)
        df = df.reset_index().drop(columns=['id']).set_index(['modelName', 'displayName', 'name', 'type'])
        df = pd.concat([df.drop(['streams'], axis=1), df['streams'].apply(pd.Series, dtype='object')], axis=1).stack()
        df = df.apply(pd.Series)
        df.index.set_names(['modelName', 'unitDisplayName', 'unitName', 'unitType', 'id'], inplace=True)
        df = df.reset_index().drop(columns=['id'])
        columns = {
            'displayName': 'streamDisplayName',
            'inOut'      : 'streamInOut',
            'name'       : 'streamName',
            'type'       : 'streamType'}
        df.rename(columns=columns, inplace=True)
        df['modelUnitName'] = df['modelName'] + '_' + df['unitName']
        model_columns = ['modelDisplayName', 'modelAbbreviation', 'modelGroup', 'modelSubgroup']
        df = pd.merge(df, self.model[model_columns], on='modelName', how='left')
        return df


    def unit_df(self):
        stream_columns = ['modelUnitName', 'unitName', 'unitDisplayName', 'modelName']
            #, 'modelDisplayName', 'modelAbbreviation', 'modelGroup', 'modelSubgroup']
        model_columns = ['modelDisplayName', 'modelAbbreviation', 'modelGroup', 'modelSubgroup']
        df = pd.merge(self.streams[stream_columns].drop_duplicates(subset='modelUnitName'),
                      self.model[model_columns],
                      on='modelName', how = 'left')
        return df

    def cost_stream_df(self):# Cost
        costStreams = pd.DataFrame()
        if "evaluated" in self.json:
            costStreams1 = pd.DataFrame(self.json["evaluated"][0][0]["coststreams1"]) \
                .set_index(['name', 'layerName'], verify_integrity=True, drop=True).drop(columns='coefficient2')
            costStreams2 = pd.DataFrame(self.json["evaluated"][0][0]["coststreams2"]) \
                .set_index(['name', 'layerName'], verify_integrity=True, drop=True).drop(columns='coefficient1')
            costStreams = pd.concat([costStreams1, costStreams2], axis=1).fillna(0).reset_index()
            costStreams['name'].replace({'_Cost': '', '_Impact': ''}, regex=True, inplace=True)
            rep = {'DefaultInvCost': 'invCost', 'DefaultOpCost': 'opCost', 'DefaultImpact': 'impact'}
            costStreams['layerName'].replace(rep, regex=True, inplace=True)
            costStreams.rename(columns={'coefficient1': '1', 'coefficient2': '2'}, inplace=True)
            costStreams = costStreams.pivot(index=['name'], columns=['layerName'], values=['1', '2']).fillna(0)
            costStreams.columns = costStreams.columns.droplevel(level=0) + costStreams.columns.droplevel(level=1)
            if 'invCost1' not in costStreams: costStreams['invCost1'] = 0
            if 'invCost2' not in costStreams: costStreams['invCost2'] = 0
            if 'opCost1' not in costStreams: costStreams['opCost1'] = 0
            if 'opCost2' not in costStreams: costStreams['opCost2'] = 0
            if 'impact1' not in costStreams: costStreams['impact1'] = 0
            if 'impact2' not in costStreams: costStreams['impact2'] = 0
            costStreams.index.names = ['locationClusterModelUnitName']
        return costStreams

    def mass_stream_df(self):
        mass_streams = pd.DataFrame()
        if "evaluated" in self.json:
            mass_streams_in = pd.DataFrame(self.json["evaluated"][0][0]["massstreamsIn"])
            mass_streams_out = pd.DataFrame(self.json["evaluated"][0][0]["massstreamsOut"])
            mass_streams_in['streamInOut'] = 'in'
            mass_streams_out['streamInOut'] = 'out'
            columns = ['flowrate', 'layerName', 'name', 'unitName', 'streamInOut']
            mass_streams = pd.concat([mass_streams_in[columns], mass_streams_out[columns]], axis=0)
            columns = {'flowrate': 'flow', 'name': 'locationClusterModelUnitStreamName',
                       'unitName': 'locationClusterModelUnitName'}
            mass_streams = mass_streams.rename(columns=columns)\
                .set_index('locationClusterModelUnitStreamName', verify_integrity=True)
            mass_streams['layerName'] =  mass_streams['layerName'].str.removeprefix('layers_')
        return mass_streams




    def heat_stream_df(self):
        df = pd.DataFrame()
        if "evaluated" in self.json:
            df = pd.DataFrame(self.json["evaluated"][0][0]["streams"])
            columns = {'name': 'locationClusterModelUnitStreamName', 'sgroup': 'groupLocationClusterModelUnitStreamName'}
            df =  df.rename(columns=columns).set_index('locationClusterModelUnitStreamName', verify_integrity=True)
        return df

    def connection_df(self):
        # Model - Connection Table
        # df_unit = df_unit(df_unit[''].isin(['Alloy', 'NaCl', 'Cullet_int']))
        df_streamIn = self.streams[self.streams['streamInOut'] == 'in']
        df_streamOut = self.streams[self.streams['streamInOut'] == 'out']
        # Inter-unit connection: Stream out(src) -> Stream in(tgt)
        df = pd.merge(df_streamOut, df_streamIn, on=['layerName'], how='left', suffixes=('_src', '_tgt'))
        # Purely (in) streams: set streamInOut_tgt "nan" to "in"
        df_in = pd.merge(df_streamOut, df_streamIn, on=['layerName'], how='right', suffixes=('_src', '_tgt'))
        d_fin = df_in[df_in['streamInOut_src'].isna()]
        df = pd.concat([df, df_in], axis=0, ignore_index=True)
        return df

    def input_df(self):
        # df = self.df[self.df['key'] == 'inputs'].drop(columns=['key']).set_index('modelName')
        # df = pd.concat([df.drop([0], axis=1), pd.DataFrame(df[0].values.tolist(), index=df.index)]).stack()
        # df.columns = ['inputs', 0]
        # df.set_index(['modelName', 'inputs'], inplace=True)

        df = self.df[self.df['key'] == 'inputs'].drop(columns=['key']).set_index('modelName')
        df = pd.concat([df.drop([0], axis=1), df[0].apply(pd.Series, dtype='object')], axis=1).stack()
        df = df.apply(pd.Series)
        df.index.set_names(['modelName', 'inputs'], inplace=True)
        return df


    def to_csv(self, directory):
        Path(directory).mkdir(parents=True, exist_ok=True)
        self.model.to_csv(Path(directory, 'model.csv'), sep='\t')
        self.layers.to_csv(Path(directory, 'layers.csv'), sep='\t')
        self.layers_error.to_csv(Path(directory, 'layers_error.csv'), sep='\t')
        self.streams.to_csv(Path(directory, 'streams.csv'), sep='\t')
        self.units.to_csv(Path(directory, 'units.csv'), sep='\t')
        self.heatStreams.to_csv(Path(directory, 'heatStreams.csv'), sep='\t')
        self.costStreams.to_csv(Path(directory, 'costStreams.csv'), sep='\t')
        self.massStreams.to_csv(Path(directory, 'massStreams.csv'), sep='\t')
        self.inputs.to_csv(Path(directory, 'inputs.csv'), sep='\t')
        self.connections.to_csv(Path(directory, 'connections.csv'), sep='\t')



class Results():

    def __init__(self, json,  units):
        self.json = json
        self.units = units
        self.flows = self.flow_df()

    def flow_df(self):
        # Results - Flows
        # layerId, layerUnit, realValue, source, target,
        # sourceCluster, sourceLocation, modelUnitName_src, modelName_src, unitName_src,
        # targetCluster, targetLocation, modelUnitName_tgt, modelName_tgt, unitName__tgt
        df = pd.DataFrame()
        if ("results" in self.json) and ("rb_shipment" in self.json["results"]):
            df = pd.DataFrame(self.json["results"]["rb_shipment"][0][0])
            # split fullname to identyfy source and target
            df[['sourceCluster', 'sourceLocation', 'modelUnitName_src']] = \
                df['source'].str.split('_', 2, expand=True)
            # supplement unit data to the source
            df = pd.merge(df, self.units.add_suffix('_src'), how="left", left_on=['modelUnitName_src'],
                          right_on=['modelUnitName_src'])
            # supplement unit data to the source
            df[['targetCluster', 'targetLocation', 'modelUnitName_tgt']] = \
                df['target'].str.split('_', 2, expand=True)
            df = pd.merge(df, self.units.add_suffix('_tgt'), how="left", left_on=['modelUnitName_tgt'],
                          right_on=['modelUnitName_tgt'])
            df = df.rename(columns={'layerId': 'layerName'})
        return df
        # Problem modelName is not defined in the header but when the ET is defined in the frontend and is not given a displayName.
        # df = pd.merge(df, df_header[['modelName', 'displayName']], how="left", left_on=['modelName_tgt'], right_on=['modelName'])
        # df.rename(columns={'displayName': 'displayName_tgt'})

    def to_csv(self, directory):
        Path(directory).mkdir(parents=True, exist_ok=True)
        self.flows.to_csv(Path(directory, 'flows.csv'), sep='\t')
